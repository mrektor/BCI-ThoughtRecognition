{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model, neighbors,datasets\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timestep=0.128040973111396;\n",
    "fc=1/timestep;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yes_dxy_contents = sio.loadmat('NIRSdxy_yes_signal.mat')\n",
    "no_dxy_contents = sio.loadmat('NIRSdxy_no_signal.mat')\n",
    "\n",
    "yes_oxy_contents = sio.loadmat('NIRSoxy_yes_signal.mat')\n",
    "no_oxy_contents = sio.loadmat('NIRSoxy_no_signal.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction(signal,feature_dictionary):\n",
    "    feature=[]\n",
    "    if(feature_dictionary[\"fft_max_frequencies\"]==1):\n",
    "        f=np.linspace(0, fc, num=signal.size)\n",
    "        spectrum= np.abs(np.fft.fft(signal));\n",
    "        fft_max_frequencies=f[np.argsort(spectrum)[-3:]]\n",
    "        feature=np.concatenate((feature,fft_max_frequencies), axis=0);\n",
    "    \n",
    "\n",
    "    if(feature_dictionary[\"mean\"]==1):\n",
    "        mean=np.mean(signal);\n",
    "        feature=np.concatenate((feature,[mean]), axis=0)\n",
    "    \n",
    "    if(feature_dictionary[\"variance\"]==1):\n",
    "        variance=np.var(signal)\n",
    "        feature=np.concatenate((feature,[variance]), axis=0)\n",
    "    #crest factor \n",
    "    if(feature_dictionary[\"crest_factor\"]==1):\n",
    "        crest_factor=np.sum(np.power(signal,2))/signal.size\n",
    "        feature=np.concatenate((feature,[crest_factor]), axis=0)\n",
    "    return np.asarray(feature)\n",
    "#.reshape([feature.size,1])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_matrix(channels, feature_dictionary,label):\n",
    "    list_train=[]\n",
    "    list_labels=[]\n",
    "    dim=channels.shape\n",
    "    for i in range (0,dim[1]):\n",
    "        single_measurement=channels[0,i]\n",
    "        dim1=single_measurement.shape\n",
    "        for j  in range (0,dim1[1]):\n",
    "            features=feature_extraction(single_measurement[:,j],feature_dictionary)\n",
    "            list_train.append([features]);\n",
    "        labels=get_labels(dim1[1],label);\n",
    "        list_labels.append([labels]);\n",
    "        \n",
    "            \n",
    "            \n",
    "    train_TX=np.concatenate(list_train)\n",
    "    labels=np.concatenate(list_labels,axis=1)\n",
    "    \n",
    "    return train_TX,labels.T.reshape(labels.size)\n",
    "\n",
    "def get_labels(number, string):\n",
    "    if(string==\"No\"):\n",
    "        return np.zeros(number)    \n",
    "    if(string==\"Yes\"):\n",
    "        return np.ones(number)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_ones(tx):\n",
    "    \"\"\"\n",
    "\tAdd column of ones to the dataset tx\n",
    "    \"\"\"\n",
    "    return np.concatenate((tx, np.ones([tx.shape[0],1])), axis=1)\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the data set x.\"\"\"\n",
    "    # Compute the mean for each column\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    # Compute the standard deviation for each column\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return np.array(x)\n",
    "\n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\" Returns the polynomial basis functions for input data x, for j=2 up to j=degree.\"\"\"\n",
    "    new_cols=np.array([x**p for p in range(2,degree+1)]).T;\n",
    "    return new_cols\n",
    "\n",
    "def add_powers(tx, degree):\n",
    "    for col in range(0,tx.shape[1]): \n",
    "            tx = np.concatenate((tx, build_poly(tx[:,col], degree)), axis=1)\n",
    "    return tx\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"Build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_logistic_regularized(Y,X, degrees, lambdas, k_fold, seed, max_iters):\n",
    "    \n",
    "    # Get the indices so that we get the k'th subgroup in test, others in train, for each k\n",
    "    k_indices = build_k_indices(Y, k_fold, seed)\n",
    "    \n",
    "    # Initialize matrix of computed accuracies for each degree and each fold\n",
    "    accuracies_train_by_fold = np.zeros([len(degrees), len(lambdas), k_fold])\n",
    "    accuracies_test_by_fold = np.zeros([len(degrees), len(lambdas), k_fold])\n",
    "\n",
    "    \n",
    "    for k in range(k_fold):\n",
    "        print('--- Fold', k, '---')\n",
    "        # Create the testing set for this fold number\n",
    "        k_index = k_indices[k] # Indices of the testing set for fold k\n",
    "        Y_cross_val_test = Y[k_index]\n",
    "        X_cross_val_test = X[k_index,:]\n",
    "        \n",
    "        \n",
    "        # Create the training set for this fold number\n",
    "        mask = np.ones(len(Y), dtype=bool) # set all elements to True\n",
    "        mask[k_index] = False # set test elements to False\n",
    "        Y_cross_val_train = Y[mask] # select only True elements (ie train elements)\n",
    "        X_cross_val_train = X[mask,:]\n",
    "       \n",
    "        # Compute the accuracies for each degree\n",
    "        accuracies_train_by_fold[:,:,k], accuracies_test_by_fold[:,:,k] = cross_validation_one_fold_logistic_regularized\\\n",
    "            (Y_cross_val_train, Y_cross_val_test, X_cross_val_train, X_cross_val_test, \\\n",
    "                                 degrees, lambdas,max_iters)\n",
    "    # Compute the mean accuracies over the folds, for each degree\n",
    "    mean_accuracies_train_by_deg = np.mean(accuracies_train_by_fold, axis=2)\n",
    "    mean_accuracies_test_by_deg = np.mean(accuracies_test_by_fold, axis=2)\n",
    "    \n",
    "    # Get the index of the best accuracy in the testing set\n",
    "    max_id_deg_test,max_id_lambda = \\\n",
    "        np.unravel_index(mean_accuracies_test_by_deg.argmax(), mean_accuracies_test_by_deg.shape)\n",
    "    \n",
    "    # Find the optimal degree and the corresponding accuracies in the training and testing sets\n",
    "    best_deg = degrees[max_id_deg_test]\n",
    "    best_lambda=lambdas[max_id_lambda]\n",
    "    best_accuracy_test = mean_accuracies_test_by_deg[max_id_deg_test,max_id_lambda]\n",
    "    corresponding_accuracy_train = mean_accuracies_train_by_deg[max_id_deg_test,max_id_lambda]\n",
    "    \n",
    "    print('Best accuracy test =', best_accuracy_test, 'with degree =', best_deg , 'lambda=',best_lambda)\n",
    "    print('Corresponding accuracy train =', corresponding_accuracy_train)\n",
    "    \n",
    "    return best_deg, best_lambda, best_accuracy_test, corresponding_accuracy_train                        \n",
    "\n",
    "\n",
    "def cross_validation_one_fold_logistic_regularized(y_cross_val_train, y_cross_val_test, tx_cross_val_train, tx_cross_val_test, \\\n",
    "                                 degrees, lambdas, max_iters):\n",
    "    \n",
    "    accuracies_train_by_deg = np.zeros([len(degrees),len(lambdas)])\n",
    "    accuracies_test_by_deg = np.zeros([len(degrees),len(lambdas)])\n",
    "    \n",
    "    # For each degree, compute the least squares weights, the predictions and the accuracies\n",
    "    for deg_id, deg in enumerate(degrees):\n",
    "        print('++ Degree', deg, '++')\n",
    "                \n",
    "        # Add powers of the chosen columns\n",
    "        len_data = tx_cross_val_train.shape[1]\n",
    "        tx_cross_val_train = add_powers(tx_cross_val_train,deg )\n",
    "        tx_cross_val_train = add_ones(tx_cross_val_train)\n",
    "        \n",
    "        tx_cross_val_test = add_powers(tx_cross_val_test,deg)\n",
    "        tx_cross_val_test = add_ones(tx_cross_val_test)\n",
    "        \n",
    "        \n",
    "        for lambda_id, single_lambda in enumerate(lambdas):\n",
    "                \n",
    "                print('>> Lambda', single_lambda, '<<')\n",
    "                # Compute the best weights on the training set\n",
    "                logreg = linear_model.LogisticRegression(C=1/single_lambda, class_weight=\"balanced\",max_iter=max_iters)\n",
    "                logreg.fit(tx_cross_val_train,y_cross_val_train )\n",
    "\n",
    "                # Compute the predictions\n",
    "                y_predicted_cross_val_train = logreg.predict(tx_cross_val_train)\n",
    "                y_predicted_cross_val_test = logreg.predict(tx_cross_val_test)\n",
    "\n",
    "\n",
    "\n",
    "                # Compute the accuracies for each degree\n",
    "                accuracies_train_by_deg[deg_id,lambda_id] = \\\n",
    "                    np.sum(y_predicted_cross_val_train == y_cross_val_train)/len(y_cross_val_train)\n",
    "                accuracies_test_by_deg[deg_id,lambda_id] = \\\n",
    "                    np.sum(y_predicted_cross_val_test == y_cross_val_test)/len(y_cross_val_test)\n",
    "\n",
    "\n",
    "                print(accuracies_test_by_deg[deg_id,lambda_id])\n",
    "        \n",
    "        \n",
    "    return accuracies_train_by_deg, accuracies_test_by_deg\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEOXY SIGNALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'no_dxy_contents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-be3e89b38174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#buildig the train matrix and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mchannels_no\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_dxy_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"no_signal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mchannels_yes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myes_dxy_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"yes_signal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'no_dxy_contents' is not defined"
     ]
    }
   ],
   "source": [
    "#buildig the train matrix and labels\n",
    "\n",
    "channels_no=no_dxy_contents[\"no_signal\"]\n",
    "channels_yes=yes_dxy_contents[\"yes_signal\"]\n",
    "\n",
    "# select which feature select\n",
    "feature_dictionary = {\n",
    "        \"fft_max_frequencies\" : 1, \n",
    "         \"mean\" : 1, \n",
    "         \"variance\" : 1,\n",
    "         \"crest_factor\" : 1\n",
    "         }\n",
    "\n",
    "\n",
    "\n",
    "train_TX_yes,labels_yes=get_train_matrix(channels_yes, feature_dictionary,\"Yes\");\n",
    "train_TX_no,labels_no=get_train_matrix(channels_no, feature_dictionary,\"No\");\n",
    "\n",
    "train_TX=np.concatenate((train_TX_yes,train_TX_no),axis=0)\n",
    "labels=np.concatenate((labels_yes,labels_no),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 6)\n",
      "--- Fold 0 ---\n",
      "++ Degree 2 ++\n",
      ">> Lambda 1e-08 <<\n",
      "0.5\n",
      ">> Lambda 1.58489319246e-07 <<\n",
      "0.5\n",
      ">> Lambda 2.51188643151e-06 <<\n",
      "0.5\n",
      ">> Lambda 3.98107170553e-05 <<\n",
      "0.5\n",
      ">> Lambda 0.00063095734448 <<\n",
      "0.5\n",
      ">> Lambda 0.01 <<\n",
      "0.5\n",
      "++ Degree 3 ++\n",
      ">> Lambda 1e-08 <<\n",
      "0.515555555556\n",
      ">> Lambda 1.58489319246e-07 <<\n",
      "0.515555555556\n",
      ">> Lambda 2.51188643151e-06 <<\n",
      "0.515555555556\n",
      ">> Lambda 3.98107170553e-05 <<\n",
      "0.515555555556\n",
      ">> Lambda 0.00063095734448 <<\n",
      "0.515555555556\n",
      ">> Lambda 0.01 <<\n",
      "0.515555555556\n",
      "--- Fold 1 ---\n",
      "++ Degree 2 ++\n",
      ">> Lambda 1e-08 <<\n",
      "0.513333333333\n",
      ">> Lambda 1.58489319246e-07 <<\n",
      "0.513333333333\n",
      ">> Lambda 2.51188643151e-06 <<\n",
      "0.513333333333\n",
      ">> Lambda 3.98107170553e-05 <<\n",
      "0.513333333333\n",
      ">> Lambda 0.00063095734448 <<\n",
      "0.513333333333\n",
      ">> Lambda 0.01 <<\n",
      "0.515555555556\n",
      "++ Degree 3 ++\n",
      ">> Lambda 1e-08 <<\n",
      "0.517777777778\n",
      ">> Lambda 1.58489319246e-07 <<\n",
      "0.517777777778\n",
      ">> Lambda 2.51188643151e-06 <<\n",
      "0.517777777778\n",
      ">> Lambda 3.98107170553e-05 <<\n",
      "0.517777777778\n",
      ">> Lambda 0.00063095734448 <<\n",
      "0.517777777778\n",
      ">> Lambda 0.01 <<\n",
      "0.517777777778\n",
      "Best accuracy test = 0.516666666667 with degree = 3 lambda= 1e-08\n",
      "Corresponding accuracy train = 0.535555555556\n"
     ]
    }
   ],
   "source": [
    "degrees = range(2,4)\n",
    "lambdas = np.logspace(-8,-2,6)\n",
    "k_fold=2\n",
    "seed = 2\n",
    "max_iters = 5000\n",
    "print(train_TX.shape)\n",
    "\n",
    "best_deg,best_lambda, best_accuracy_test, corresponding_accuracy_train = \\\n",
    "        cross_validation_logistic_regularized(labels,train_TX, degrees, lambdas, k_fold, seed, max_iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47333333333333333\n",
      "0.4411111111111111\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(train_TX, labels)\n",
    "predicted_labels= logreg.predict(train_TX)\n",
    "logreg_accuracy=get_accuracy(predicted_labels, labels)\n",
    "print(logreg_accuracy)\n",
    "\n",
    "\n",
    "#KNN \n",
    "clf = neighbors.KNeighborsClassifier(200)\n",
    "clf.fit(train_TX, labels)\n",
    "predicted_labels= clf.predict(train_TX)\n",
    "KNN_accuracy=get_accuracy(predicted_labels, labels)\n",
    "print(KNN_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
