{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model, neighbors,datasets\n",
    "from lib.helpers import *\n",
    "from sklearn import svm\n",
    "from lib.cross_validations_lib import *\n",
    "#import peakutils\n",
    "import scipy.signal as signal\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(signal,feature_dictionary,f_peak):\n",
    "    feature=[]\n",
    "    \n",
    "    if(feature_dictionary[\"fft_peak_frequencies\"]==1):\n",
    "        feature=np.concatenate((feature,f_peak.reshape(f_peak.shape[1])), axis=0);\n",
    "        \n",
    "    if(feature_dictionary[\"mean\"]==1):\n",
    "        mean=np.mean(signal);\n",
    "        feature=np.concatenate((feature,[mean]), axis=0)\n",
    "    \n",
    "    if(feature_dictionary[\"variance\"]==1):\n",
    "        variance=np.var(signal)\n",
    "        feature=np.concatenate((feature,[variance]), axis=0)\n",
    "    #crest factor \n",
    "    if(feature_dictionary[\"crest_factor\"]==1):\n",
    "        crest_factor=np.sum(np.power(signal,2))/signal.size\n",
    "        feature=np.concatenate((feature,[crest_factor]), axis=0)\n",
    "    \n",
    "    return np.asarray(feature)\n",
    "#.reshape([feature.size,1])\n",
    "\n",
    "\n",
    "def split_matrix_two_blocks(y, percentage1, percentage2, seed):\n",
    "    \"\"\"Build k indices for k-fold.\"\"\"\n",
    "    if(percentage1+percentage2==1):\n",
    "        num_row = len(y)\n",
    "        print(num_row)\n",
    "        interval_1 = int(percentage1*num_row);\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.permutation(num_row);\n",
    "        first_indices = indices[0:interval_1];\n",
    "        second_indices = indices[interval_1:num_row];\n",
    "        return [np.array(first_indices),np.array(second_indices)]\n",
    "    else:\n",
    "        print('>>>>>>>>>>>ERROR:Not valid splitting percentage')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_signals(channels):\n",
    "    time_instances=[];\n",
    "    dim=channels.shape;\n",
    "    #find the length min of the signal in the specified temporal instance\n",
    "    # for NIRS signal ==80 \n",
    "    length_min=len(channels[0,1]);\n",
    "    for i in range (0,dim[1]):\n",
    "        single_measurement=channels[0,i];\n",
    "        single_length=single_measurement.shape[0]\n",
    "        if(single_length<length_min):\n",
    "                length_min=single_length;\n",
    "    #export the signals\n",
    "    for i in range (0,dim[1]):\n",
    "        single_measurement=channels[0,i];\n",
    "        dim1=single_measurement.shape;\n",
    "        time_instance=[];\n",
    "        for j  in range (0,dim1[1]):\n",
    "            if(len(single_measurement[:,j])>length_min):\n",
    "                single_signal=single_measurement[:,j][0:length_min]\n",
    "            else:\n",
    "                single_signal=single_measurement[:,j]\n",
    "            #put in a list \n",
    "            time_instance.append(np.asarray(single_signal).reshape(len(single_signal),1).T);\n",
    "       # create the matrix of the signals per a single time instance \n",
    "        time_instance=np.concatenate(time_instance);\n",
    "        time_instances.append(time_instance);   \n",
    "    return time_instances;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_feature_matrix_and_labels(channel_structure, feature_dictionary,label,features_extracted):\n",
    "    list_train=[]\n",
    "    list_labels=[]\n",
    "    cont=0;\n",
    "    peak_signal=features_extracted[\"peak_signal\"]\n",
    "    f_peak_signal=peak_signal[\"f_cell\"];\n",
    "    \n",
    "    for time_instance in channel_structure:\n",
    "        dim1=time_instance.shape\n",
    "        for j  in range (0,dim1[0]):\n",
    "            features=feature_extraction(time_instance[j,:],feature_dictionary,f_peak_signal[0,cont])\n",
    "            list_train.append([features]);\n",
    "            cont=cont+1;\n",
    "        labels=get_labels(dim1[0],label);\n",
    "        list_labels.append([labels]);\n",
    "        \n",
    "    train_TX=np.concatenate(list_train)\n",
    "    labels=np.concatenate(list_labels,axis=1)\n",
    "    \n",
    "    return train_TX,labels.T.reshape(labels.size)\n",
    "\n",
    "\n",
    "def get_feature_matrix(channels,feature_dictionary,features_extracted):\n",
    "    list_train=[]\n",
    "    list_labels=[]\n",
    "    cont=0;\n",
    "    peak_signal=features_extracted[\"peak_signal\"]\n",
    "    f_peak_signal=peak_signal[\"f_cell\"];\n",
    "    for time_instance in channel_structure:\n",
    "        dim1=time_instace.shape\n",
    "        print(dim1)\n",
    "        for j  in range (0,dim1[0]):\n",
    "            features=feature_extraction(time_instance[j,:],feature_dictionary,f_peak_signal[0,cont])\n",
    "            list_train.append([features]);\n",
    "            cont=cont+1;\n",
    "    train_TX=np.concatenate(list_train)\n",
    "    return train_TX\n",
    "\n",
    "def get_labels(number, string):\n",
    "    if(string==\"No\"):\n",
    "        return np.zeros(number)    \n",
    "    if(string==\"Yes\"):\n",
    "        return np.ones(number)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(780, 6)\n"
     ]
    }
   ],
   "source": [
    "yes_EEG_contents = sio.loadmat('EEGyes_signal.mat')\n",
    "no_EEG_contents = sio.loadmat('EEGno_signal.mat')\n",
    "\n",
    "\n",
    "channels_no_EEG=no_EEG_contents[\"no_signal\"]\n",
    "channels_yes_EEG=yes_EEG_contents[\"yes_signal\"]\n",
    "\n",
    "pick_yes_EEG_contents = sio.loadmat('pick_EEG_yes_signal.mat')\n",
    "pick_no_EEG_contents = sio.loadmat('pick_EEG_no_signal.mat')\n",
    "\n",
    "# select which feature select\n",
    "feature_dictionary = {\n",
    "        \"fft_peak_frequencies\" : 0, \n",
    "         \"mean\" : 0, \n",
    "         \"variance\" : 0,\n",
    "         \"crest_factor\" : 0\n",
    "         }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "channels_structure_yes_EEG=export_signals(channels_yes_EEG)\n",
    "channels_structure_no_EEG=export_signals(channels_no_EEG)\n",
    "\n",
    "\n",
    "features_extracted={\n",
    "    \"peak_signal\" : pick_yes_EEG_contents,\n",
    "}\n",
    "\n",
    "\n",
    "train_TX_yes_EEG,EEG_yes_labels=get_feature_matrix_and_labels(channels_structure_yes_EEG, feature_dictionary,\"Yes\",features_extracted);\n",
    "\n",
    "features_extracted={\n",
    "    \"peak_signal\" : pick_no_EEG_contents,\n",
    "}\n",
    "\n",
    "\n",
    "train_TX_no_EEG,EEG_no_labels=get_feature_matrix_and_labels(channels_structure_no_EEG, feature_dictionary,\"No\",features_extracted);\n",
    "\n",
    "\n",
    "\n",
    "train_TX=np.concatenate((train_TX_yes_EEG,train_TX_no_EEG,),axis=0)\n",
    "\n",
    "print(train_TX.shape)\n",
    "labels=np.concatenate((EEG_yes_labels,EEG_no_labels),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_parameters= np.linspace(0.1,5,10)\n",
    "k_fold=5 # number of k sub-folders to divide the set\n",
    "seed = 2\n",
    "max_iters = 100\n",
    "kernel_types=['linear', 'rbf', 'sigmoid']\n",
    "print(train_TX.shape)\n",
    "\n",
    "best_C, best_kernel_type ,best_accuracy_test, corresponding_accuracy_train = \\\n",
    "        cross_validation_SVM(labels,train_TX, C_parameters, kernel_types, k_fold, seed, max_iters)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780\n"
     ]
    }
   ],
   "source": [
    "best_C=0.1;\n",
    "best_kernel_type='linear'\n",
    "print(train_TX.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780\n",
      "0.474358974359\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "best_C=0.1;\n",
    "seed=[123,12,3,4,2,1]\n",
    "best_kernel_type='linear'\n",
    "dataset_length=train_TX.shape[0];\n",
    "\n",
    "[i1,i2]=split_matrix_two_blocks(train_TX,0.7,0.3,2)\n",
    "train=train_TX[i1,:]\n",
    "labels_train=labels[i1]\n",
    "test= train_TX[i2,:]\n",
    "labels_test=labels[i2]\n",
    "clf = svm.SVC(C=best_C, cache_size=200, class_weight=None, coef0=0.0,\n",
    "        decision_function_shape='ovr', degree=3, gamma='auto', kernel=best_kernel_type,\n",
    "        max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "        tol=0.001, verbose=False)\n",
    "\n",
    "\n",
    "clf.fit(train, labels_train)  \n",
    "predicted_labels= clf.predict(test)\n",
    "\n",
    "\n",
    "SVM_accuracy=get_accuracy(predicted_labels, labels_test)\n",
    "print(SVM_accuracy)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
