{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model, neighbors, datasets\n",
    "from sklearn import svm\n",
    "import scipy.signal as signal\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_matrix_two_blocks(y, percentage1, percentage2, seed):\n",
    "    \"\"\"Build k indices for k-fold.\"\"\"\n",
    "    if(percentage1+percentage2==1):\n",
    "        num_row = len(y)\n",
    "        #print(num_row)\n",
    "        interval_1 = int(percentage1*num_row);\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.permutation(num_row);\n",
    "        first_indices = indices[0:interval_1];\n",
    "        second_indices = indices[interval_1:num_row];\n",
    "        return [np.array(first_indices),np.array(second_indices)]\n",
    "    else:\n",
    "        print('>>>>>>>>>>>ERROR:Not valid splitting percentage')\n",
    "        \n",
    "        \n",
    "##\n",
    "## This function reutrn a list of matrices. Each matrix correspond to a question instance in which each row is a channel, and in the coloumn it develop the signal in time\n",
    "## The function also manage to standardize the time length\n",
    "def channels_to_vector(channels): \n",
    "    time_instances=[];\n",
    "    dim=channels.shape;\n",
    "    #find the length min of the signal in the specified temporal instance\n",
    "    length_min=len(channels[0,1]);\n",
    "    for i in range (0,dim[1]):\n",
    "        single_measurement=channels[0,i];\n",
    "        single_length=single_measurement.shape[0]\n",
    "        if(single_length<length_min):\n",
    "                length_min=single_length;\n",
    "    #export the signals\n",
    "    for i in range (0,dim[1]):\n",
    "        single_measurement=channels[0,i];\n",
    "        dim1=single_measurement.shape;\n",
    "        time_instance=[];\n",
    "        for j  in range (0,dim1[1]):\n",
    "            if(len(single_measurement[:,j])>length_min):\n",
    "                single_signal=single_measurement[:,j][0:length_min]\n",
    "            else:\n",
    "                single_signal=single_measurement[:,j]\n",
    "            #put in a list \n",
    "            time_instance.append(np.asarray(single_signal).reshape(len(single_signal),1).T);\n",
    "       # create the matrix of the signals per a single time instance \n",
    "        time_instance=np.concatenate(time_instance);\n",
    "        time_instances.append(time_instance);   \n",
    "    return time_instances;\n",
    "\n",
    "\n",
    "##\n",
    "# Create the train data matrix\n",
    "##\n",
    "## usage\n",
    "def get_feature_matrix_and_labels(channel_structure,label,features_extracted,connectivity_feature):\n",
    "    list_train=[]\n",
    "    list_labels=[]\n",
    "    cont=0;\n",
    "    index_connectivity=0;\n",
    "    list_row=[]\n",
    "    \n",
    "    for time_instance in channel_structure:\n",
    "        dim1=time_instance.shape\n",
    "        #indipendent_components=extract_ICs(time_instance,n_ICA_components);\n",
    "        for j  in range (0,dim1[0]):\n",
    "           \n",
    "            features=features_extracted[cont,:];\n",
    "            list_row.append(features);\n",
    "            cont=cont+1;\n",
    "        list_row.append(connectivity_feature[index_connectivity,:]);\n",
    "        index_connectivity=index_connectivity+1;\n",
    "        labels=get_labels(1,label);\n",
    "        feature_row=np.concatenate(list_row);\n",
    "        list_train.append(feature_row.reshape(len(feature_row),1).T)\n",
    "        list_labels.append(labels);\n",
    "        list_row=[]\n",
    "        \n",
    "    train_TX=np.concatenate(list_train)\n",
    "    labels=np.concatenate(list_labels,axis=0)\n",
    "    \n",
    "    return train_TX,labels.T.reshape(labels.size)\n",
    "\n",
    "\n",
    "### Description\n",
    "def get_labels(number, string):\n",
    "    if(string==\"No\"):\n",
    "        return np.zeros(number)    \n",
    "    if(string==\"Yes\"):\n",
    "        return np.ones(number)\n",
    "    \n",
    "## description\n",
    "def select_features(weights,matrix,th):\n",
    "    cont=0;\n",
    "    i=0;\n",
    "    while(cont<len(weights)):\n",
    "        if(weights[cont]<th):\n",
    "\n",
    "            mask = np.ones(matrix.shape[1], dtype=bool)\n",
    "            mask[i] = False\n",
    "            matrix=matrix[:,mask]\n",
    "        else:\n",
    "            i=i+1;\n",
    "        cont=cont+1;\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def get_accuracy(predicted_labels, true_labels):\n",
    "     if (predicted_labels.size == true_labels.size):\n",
    "        return  np.sum(predicted_labels ==  true_labels )/len( true_labels)\n",
    " \n",
    "    \n",
    "    \n",
    "def extract_dataset(destination_folder):\n",
    "    #Import data from mat files\n",
    "    old_path=os.getcwd()\n",
    "    os.chdir(destination_folder)\n",
    "    yes_EEG_contents = sio.loadmat('EEGyes.mat')\n",
    "    no_EEG_contents = sio.loadmat('EEGno.mat')\n",
    "\n",
    "    channels_no_EEG=no_EEG_contents[\"EEGno\"]\n",
    "    channels_yes_EEG=yes_EEG_contents[\"EEGyes\"]\n",
    "\n",
    "    #Features Loading\n",
    "    features_extracted_yes   = sio.loadmat('FeaturesYes.mat')['FeaturesYes']\n",
    "    features_extracted_no    = sio.loadmat('FeaturesNO.mat')['FeaturesNo']\n",
    "    connectivity_feature_yes = sio.loadmat('ConnectivityFeaturesYes.mat')['ConnectivityFeaturesYes']\n",
    "    connectivity_feature_no  = sio.loadmat('ConnectivityFeaturesNo.mat')['ConnectivityFeaturesNo']\n",
    "\n",
    "    channels_structure_yes_EEG = channels_to_vector(channels_yes_EEG)\n",
    "    channels_structure_no_EEG  = channels_to_vector(channels_no_EEG)\n",
    "\n",
    "    ##Structuring of the data:\n",
    "    #the code below create the train matrix with respect to the signal given in \"channel_structure\" but using the features contained in \"features_extracted*\" and in \"connettivity_feature*\".\n",
    "    feature_dataset_yes_EEG, EEG_yes_labels = get_feature_matrix_and_labels(channels_structure_yes_EEG,\"Yes\",features_extracted_yes,connectivity_feature_yes);\n",
    "\n",
    "    feature_dataset_no_EEG, EEG_no_labels = get_feature_matrix_and_labels(channels_structure_no_EEG,\"No\",features_extracted_no,connectivity_feature_no);\n",
    "\n",
    "    #Merge the labeled data\n",
    "    feature_dataset_full = np.concatenate((feature_dataset_yes_EEG, feature_dataset_no_EEG), axis=0 )\n",
    "    labels = np.concatenate((EEG_yes_labels,EEG_no_labels), axis=0)\n",
    "    os.chdir(old_path)\n",
    "\n",
    "\n",
    "    return feature_dataset_full,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained: Classifier_Day1\n",
      "Accuracy on dataset in:./DataDay1\n",
      "1.0\n",
      "Accuracy on dataset in:./DataDay2\n",
      "0.375\n",
      "Accuracy on dataset in:./DataDay4\n",
      "0.4125\n",
      "Accuracy on dataset in:./DataDay5\n",
      "0.883333333333\n",
      "Accuracy on dataset in:./DataDay6\n",
      "0.675\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "########################### INPUT PARAMETERS########\n",
    "\n",
    "#Select the traine  classifier\n",
    "\n",
    "classifier_folder= 'ModelDay1'\n",
    "classifier_name='Classifier_Day1'\n",
    "reducer_name='reducer_Day1'\n",
    "\n",
    "#Select the input folders to test the model\n",
    "Input_Path_Folders=['./DataDay1','./DataDay2','./DataDay4','./DataDay5','./DataDay6']\n",
    "\n",
    "###################################################\n",
    "\n",
    "old_path=os.getcwd()\n",
    "os.chdir(classifier_folder)\n",
    "clf = joblib.load(classifier_name+'.pkl') \n",
    "reducer=joblib.load(reducer_name+'.pkl') \n",
    "os.chdir(old_path)\n",
    "\n",
    "\n",
    "print('Model trained: '+classifier_name)\n",
    "for single_input in Input_Path_Folders:\n",
    "    print('Accuracy on dataset in:' + single_input)\n",
    "    [feature_dataset_full,labels]=extract_dataset(single_input)\n",
    "    dataset_reduced=reducer.fit_transform(feature_dataset_full,labels)\n",
    "    scaler= StandardScaler()\n",
    "    dataset_reduced = scaler.fit_transform(dataset_reduced)\n",
    "    predicted_label=clf.predict(dataset_reduced)\n",
    "    print(get_accuracy(predicted_label,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained: Classifier_Day2\n",
      "Accuracy on dataset in:./DataDay1\n",
      "0.6125\n",
      "Accuracy on dataset in:./DataDay2\n",
      "0.9875\n",
      "Accuracy on dataset in:./DataDay4\n",
      "0.5625\n",
      "Accuracy on dataset in:./DataDay5\n",
      "0.3\n",
      "Accuracy on dataset in:./DataDay6\n",
      "0.4125\n"
     ]
    }
   ],
   "source": [
    "# INPUT PARAMETERS \n",
    "\n",
    "#Select the traine  classifier\n",
    "\n",
    "classifier_folder= 'ModelDay2'\n",
    "classifier_name='Classifier_Day2'\n",
    "reducer_name='reducer_Day2'\n",
    "\n",
    "#Select the input folders to test the model\n",
    "Input_Path_Folders=['./DataDay1','./DataDay2','./DataDay4','./DataDay5','./DataDay6']\n",
    "\n",
    "old_path=os.getcwd()\n",
    "os.chdir(classifier_folder)\n",
    "clf = joblib.load(classifier_name+'.pkl') \n",
    "reducer=joblib.load(reducer_name+'.pkl') \n",
    "os.chdir(old_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Model trained: '+classifier_name)\n",
    "for single_input in Input_Path_Folders:\n",
    "    print('Accuracy on dataset in:' + single_input)\n",
    "    [feature_dataset_full,labels]=extract_dataset(single_input)\n",
    "    dataset_reduced=reducer.fit_transform(feature_dataset_full,labels)\n",
    "    scaler= StandardScaler()\n",
    "    dataset_reduced = scaler.fit_transform(dataset_reduced)\n",
    "    predicted_label=clf.predict(dataset_reduced)\n",
    "    print(get_accuracy(predicted_label,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained: Classifier_Day4\n",
      "Accuracy on dataset in:./DataDay1\n",
      "0.65\n",
      "Accuracy on dataset in:./DataDay2\n",
      "0.6125\n",
      "Accuracy on dataset in:./DataDay4\n",
      "0.9875\n",
      "Accuracy on dataset in:./DataDay5\n",
      "0.633333333333\n",
      "Accuracy on dataset in:./DataDay6\n",
      "0.425\n"
     ]
    }
   ],
   "source": [
    "# INPUT PARAMETERS \n",
    "\n",
    "#Select the traine  classifier\n",
    "\n",
    "classifier_folder= 'ModelDay4'\n",
    "classifier_name='Classifier_Day4'\n",
    "reducer_name='reducer_Day4'\n",
    "\n",
    "#Select the input folders to test the model\n",
    "Input_Path_Folders=['./DataDay1','./DataDay2','./DataDay4','./DataDay5','./DataDay6']\n",
    "\n",
    "old_path=os.getcwd()\n",
    "os.chdir(classifier_folder)\n",
    "clf = joblib.load(classifier_name+'.pkl') \n",
    "reducer=joblib.load(reducer_name+'.pkl') \n",
    "os.chdir(old_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Model trained: '+classifier_name)\n",
    "for single_input in Input_Path_Folders:\n",
    "    print('Accuracy on dataset in:' + single_input)\n",
    "    [feature_dataset_full,labels]=extract_dataset(single_input)\n",
    "    dataset_reduced=reducer.fit_transform(feature_dataset_full,labels)\n",
    "    scaler= StandardScaler()\n",
    "    dataset_reduced = scaler.fit_transform(dataset_reduced)\n",
    "    predicted_label=clf.predict(dataset_reduced)\n",
    "    print(get_accuracy(predicted_label,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained: Classifier_Day5\n",
      "Accuracy on dataset in:./DataDay1\n",
      "0.475\n",
      "Accuracy on dataset in:./DataDay2\n",
      "0.5\n",
      "Accuracy on dataset in:./DataDay4\n",
      "0.5125\n",
      "Accuracy on dataset in:./DataDay5\n",
      "1.0\n",
      "Accuracy on dataset in:./DataDay6\n",
      "0.6375\n"
     ]
    }
   ],
   "source": [
    "# INPUT PARAMETERS \n",
    "\n",
    "#Select the train  classifier\n",
    "\n",
    "classifier_folder= 'ModelDay5'\n",
    "classifier_name='Classifier_Day5'\n",
    "reducer_name='reducer_Day5'\n",
    "\n",
    "#Select the folders to test the model\n",
    "Input_Path_Folders=['./DataDay1','./DataDay2','./DataDay4','./DataDay5','./DataDay6']\n",
    "\n",
    "old_path=os.getcwd()\n",
    "os.chdir(classifier_folder)\n",
    "clf = joblib.load(classifier_name+'.pkl') \n",
    "reducer=joblib.load(reducer_name+'.pkl') \n",
    "os.chdir(old_path)\n",
    "\n",
    "\n",
    "\n",
    "print('Model trained: '+classifier_name)\n",
    "for single_input in Input_Path_Folders:\n",
    "    print('Accuracy on dataset in:' + single_input)\n",
    "    [feature_dataset_full,labels]=extract_dataset(single_input)\n",
    "    dataset_reduced=reducer.fit_transform(feature_dataset_full,labels)\n",
    "    scaler= StandardScaler()\n",
    "    dataset_reduced = scaler.fit_transform(dataset_reduced)\n",
    "    predicted_label=clf.predict(dataset_reduced)\n",
    "    print(get_accuracy(predicted_label,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained: Classifier_Day6\n",
      "Accuracy on dataset in:./DataDay1\n",
      "0.5625\n",
      "Accuracy on dataset in:./DataDay2\n",
      "0.275\n",
      "Accuracy on dataset in:./DataDay4\n",
      "0.475\n",
      "Accuracy on dataset in:./DataDay5\n",
      "0.916666666667\n",
      "Accuracy on dataset in:./DataDay6\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# INPUT PARAMETERS \n",
    "\n",
    "#Select the train  classifier\n",
    "\n",
    "classifier_folder= 'ModelDay6'\n",
    "classifier_name='Classifier_Day6'\n",
    "reducer_name='reducer_Day6'\n",
    "\n",
    "#Select the input folders to test the modle\n",
    "Input_Path_Folders=['./DataDay1','./DataDay2','./DataDay4','./DataDay5','./DataDay6']\n",
    "\n",
    "old_path=os.getcwd()\n",
    "os.chdir(classifier_folder)\n",
    "clf = joblib.load(classifier_name+'.pkl') \n",
    "reducer=joblib.load(reducer_name+'.pkl') \n",
    "os.chdir(old_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Model trained: '+classifier_name)\n",
    "for single_input in Input_Path_Folders:\n",
    "    print('Accuracy on dataset in:' + single_input)\n",
    "    [feature_dataset_full,labels]=extract_dataset(single_input)\n",
    "    dataset_reduced=reducer.fit_transform(feature_dataset_full,labels)\n",
    "    scaler= StandardScaler()\n",
    "    dataset_reduced = scaler.fit_transform(dataset_reduced)\n",
    "    predicted_label=clf.predict(dataset_reduced)\n",
    "    print(get_accuracy(predicted_label,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
