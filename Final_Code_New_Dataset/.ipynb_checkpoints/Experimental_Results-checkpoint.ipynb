{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model, neighbors, datasets\n",
    "from sklearn import svm\n",
    "import scipy.signal as signal\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.externals import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_matrix_two_blocks(y, percentage1, percentage2, seed):\n",
    "    \"\"\"Build k indices for k-fold.\"\"\"\n",
    "    if(percentage1+percentage2==1):\n",
    "        num_row = len(y)\n",
    "        #print(num_row)\n",
    "        interval_1 = int(percentage1*num_row);\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.permutation(num_row);\n",
    "        first_indices = indices[0:interval_1];\n",
    "        second_indices = indices[interval_1:num_row];\n",
    "        return [np.array(first_indices),np.array(second_indices)]\n",
    "    else:\n",
    "        print('>>>>>>>>>>>ERROR:Not valid splitting percentage')\n",
    "        \n",
    "        \n",
    "##\n",
    "## This function reutrn a list of matrices. Each matrix correspond to a question instance in which each row is a channel, and in the coloumn it develop the signal in time\n",
    "## The function also manage to standardize the time length\n",
    "def channels_to_vector(channels): \n",
    "    time_instances=[];\n",
    "    dim=channels.shape;\n",
    "    #find the length min of the signal in the specified temporal instance\n",
    "    length_min=len(channels[0,1]);\n",
    "    for i in range (0,dim[1]):\n",
    "        single_measurement=channels[0,i];\n",
    "        single_length=single_measurement.shape[0]\n",
    "        if(single_length<length_min):\n",
    "                length_min=single_length;\n",
    "    #export the signals\n",
    "    for i in range (0,dim[1]):\n",
    "        single_measurement=channels[0,i];\n",
    "        dim1=single_measurement.shape;\n",
    "        time_instance=[];\n",
    "        for j  in range (0,dim1[1]):\n",
    "            if(len(single_measurement[:,j])>length_min):\n",
    "                single_signal=single_measurement[:,j][0:length_min]\n",
    "            else:\n",
    "                single_signal=single_measurement[:,j]\n",
    "            #put in a list \n",
    "            time_instance.append(np.asarray(single_signal).reshape(len(single_signal),1).T);\n",
    "       # create the matrix of the signals per a single time instance \n",
    "        time_instance=np.concatenate(time_instance);\n",
    "        time_instances.append(time_instance);   \n",
    "    return time_instances;\n",
    "\n",
    "\n",
    "##\n",
    "# Create the train data matrix\n",
    "##\n",
    "## usage\n",
    "def get_feature_matrix_and_labels(channel_structure,label,features_extracted,connectivity_feature):\n",
    "    list_train=[]\n",
    "    list_labels=[]\n",
    "    cont=0;\n",
    "    index_connectivity=0;\n",
    "    list_row=[]\n",
    "    \n",
    "    for time_instance in channel_structure:\n",
    "        dim1=time_instance.shape\n",
    "        #indipendent_components=extract_ICs(time_instance,n_ICA_components);\n",
    "        for j  in range (0,dim1[0]):\n",
    "           \n",
    "            features=features_extracted[cont,:];\n",
    "            list_row.append(features);\n",
    "            cont=cont+1;\n",
    "        list_row.append(connectivity_feature[index_connectivity,:]);\n",
    "        index_connectivity=index_connectivity+1;\n",
    "        labels=get_labels(1,label);\n",
    "        feature_row=np.concatenate(list_row);\n",
    "        list_train.append(feature_row.reshape(len(feature_row),1).T)\n",
    "        list_labels.append(labels);\n",
    "        list_row=[]\n",
    "        \n",
    "    train_TX=np.concatenate(list_train)\n",
    "    labels=np.concatenate(list_labels,axis=0)\n",
    "    \n",
    "    return train_TX,labels.T.reshape(labels.size)\n",
    "\n",
    "\n",
    "### Description\n",
    "def get_labels(number, string):\n",
    "    if(string==\"No\"):\n",
    "        return np.zeros(number)    \n",
    "    if(string==\"Yes\"):\n",
    "        return np.ones(number)\n",
    "    \n",
    "## description\n",
    "def select_features(weights,matrix,th):\n",
    "    cont=0;\n",
    "    i=0;\n",
    "    while(cont<len(weights)):\n",
    "        if(weights[cont]<th):\n",
    "\n",
    "            mask = np.ones(matrix.shape[1], dtype=bool)\n",
    "            mask[i] = False\n",
    "            matrix=matrix[:,mask]\n",
    "        else:\n",
    "            i=i+1;\n",
    "        cont=cont+1;\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def get_accuracy(predicted_labels, true_labels):\n",
    "     if (predicted_labels.size == true_labels.size):\n",
    "        return  np.sum(predicted_labels ==  true_labels )/len( true_labels)\n",
    "    \n",
    "    \n",
    "\n",
    "def classification_SVM_experiments_std(X, Y, classifier, fraction_train_test, num_experiments,Model_Name,save_model):\n",
    "    \n",
    "    seed=range(num_experiments)\n",
    "    svm_total_acc_test  = []\n",
    "    svm_total_acc_train = [] \n",
    "    dataset_length=X.shape[0];\n",
    "    \n",
    "    for single_seed in seed:\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        [i1,i2]=split_matrix_two_blocks(X, fraction_train_test, 1-fraction_train_test,single_seed)\n",
    "\n",
    "        \n",
    "        train =X[i1,:]\n",
    "        labels_train=Y[i1]\n",
    "        \n",
    "        test = X[i2,:]\n",
    "        labels_test=Y[i2]\n",
    "        \n",
    "        scaler.fit(train)\n",
    "        train = scaler.transform(train)\n",
    "        test = scaler.transform(test)\n",
    "  \n",
    "        clf=classifier\n",
    "        clf.fit(train, labels_train)  \n",
    "\n",
    "        \n",
    "        #Accuracy on test\n",
    "        predicted_labels_test = clf.predict(test)\n",
    "        SVM_accuracy_test = get_accuracy(predicted_labels_test, labels_test)\n",
    "        svm_total_acc_test.append(SVM_accuracy_test)\n",
    "        \n",
    "        if(save_model==1):\n",
    "            old_path=os.getcwd()\n",
    "            os.chdir('Model'+Model_Name)\n",
    "            joblib.dump(clf, 'Classifier_'+Model_Name+'.pkl') \n",
    "            os.chdir(old_path)\n",
    "    \n",
    "        #Accuracy on train\n",
    "        predicted_labels_train = clf.predict(train)\n",
    "        SVM_accuracy_train = get_accuracy(predicted_labels_train, labels_train)\n",
    "        svm_total_acc_train.append(SVM_accuracy_train)\n",
    "        #print(\"Accuracy: \"+ str(SVM_accuracy) + \"; iteration  \" + str(single_seed) )\n",
    "    return svm_total_acc_test, svm_total_acc_train\n",
    "\n",
    "def performance_assesment_fraction_std(X, Y, num_experiment, classifier,Model_Name):\n",
    "    fracs = np.linspace(0.25,0.9,X.shape[0])\n",
    "    accuracy_test_mean  = []\n",
    "    accuracy_test_std   = []\n",
    "    accuracy_train_mean = []\n",
    "    accuracy_train_std  = []\n",
    "\n",
    "    for frac_tr_te in fracs:\n",
    "        #print(\"Evaluation progress: \" + str(int((frac_tr_te-fracs[0])/(fracs[-1]-fracs[0])*100)) + \" %\")\n",
    "        acc_test, acc_train = classification_SVM_experiments_std(X, Y, classifier, frac_tr_te, num_experiment,Model_Name,0)\n",
    "        #saving of metrics of interest\n",
    "        accuracy_test_mean.append(np.mean(acc_test))\n",
    "        accuracy_test_std.append(np.std(acc_test))\n",
    "        accuracy_train_mean.append(np.mean(acc_train))\n",
    "        accuracy_train_std.append(np.std(acc_train))\n",
    "    \n",
    "    frac_tr_te=0.95;\n",
    "    acc_test, acc_train =classification_SVM_experiments_std(X, Y, classifier, frac_tr_te, 1,Model_Name,1)\n",
    "    #plot the figure\n",
    "    plt.figure(figsize=(10, 7), dpi=80)\n",
    "    plt.errorbar(np.floor(fracs*60), accuracy_test_mean, yerr=accuracy_test_std, label=\"Error bars plot\", fmt=\"s-\",  linewidth=3)\n",
    "    plt.errorbar(np.floor(fracs*60), accuracy_train_mean, yerr=accuracy_train_std, label=\"Error bars plot\", fmt=\"s-\",  linewidth=3)\n",
    "    plt.grid(b=True, which='major', color='k', linestyle='--', alpha = 0.4)\n",
    "    plt.minorticks_on()\n",
    "    plt.title('SVM perfomances over different train/test dataset of reduced features')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Train instances considered')\n",
    "\n",
    "    plt.legend(['Test Accuracy', 'Train Accuracy'], loc=4)\n",
    "    name='train_test_acc_fine_tuned_IID_'+Model_Name+'.eps'\n",
    "    plt.savefig(name, format='eps')\n",
    "    plt.show()\n",
    "    \n",
    "def split_train_test_temporal(X, y, n):\n",
    "    #n is the number of instances of yes and no, n>1\n",
    "    half = int(X.shape[0]/2)\n",
    "    if (n>half): n=half\n",
    "    X_tr = np.concatenate(( X[0:n,:] , X[half:half+n,:] ))\n",
    "    X_te = np.concatenate(( X[n:half,:], X[half+n:,:] ))\n",
    "    y_tr = np.concatenate(( y[0:n]   , y[half:half+n]   ))\n",
    "    y_te = np.concatenate(( y[n:half]  , y[half+n:]   ))\n",
    "    \n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "def get_experimental_sequence(X, y, order, c):\n",
    "    idx_tr = []\n",
    "    y_tr = []\n",
    "    half = int(X.shape[0]/2)\n",
    "    #c = number of minimum instances\n",
    "    i=0\n",
    "    j=0\n",
    "    for element in order:\n",
    "        if have_two_classes(y_tr) and len(y_tr)>c:\n",
    "            break\n",
    "        if element==1: # 1 is YES\n",
    "            idx_tr.append(i)\n",
    "            #X_tr.append(X[i,:])\n",
    "            y_tr.append(y[i])\n",
    "            i+=1\n",
    "        else: # 0 is NO\n",
    "            idx_tr.append(j+half)\n",
    "            #X_tr.append(X[half+j,:])\n",
    "            y_tr.append(y[half+j])\n",
    "            j+=1\n",
    "            \n",
    "    X_tr = X[idx_tr,:]\n",
    "    y_tr = y[idx_tr]\n",
    "    \n",
    "    idx_te = [k for k in range(X.shape[0])]\n",
    "    for l in idx_tr:\n",
    "        idx_te.remove(l)\n",
    "    X_te = X[idx_te,:]\n",
    "    y_te = y[idx_te]\n",
    "    \n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "def have_two_classes(y):\n",
    "    have_zero = False\n",
    "    have_one = False\n",
    "    for element in y: \n",
    "        if element==1: have_zero = True\n",
    "    for element in y: \n",
    "        if element==0: have_one = True\n",
    "    return have_zero & have_one\n",
    "\n",
    "\n",
    "\n",
    "def performance_assesment_instances_std(X, y, order, Model_Name):\n",
    "    feature_reduced = X\n",
    "    labels = y\n",
    "    #fracs = np.linspace(0.2,0.9,60)\n",
    "    tot_perf = []\n",
    "    start = 4\n",
    "    ran = range(start,int(X.shape[0]/2))\n",
    "\n",
    "    for n in ran:\n",
    "     \n",
    "        X_tr, X_te, y_tr, y_te = get_experimental_sequence(feature_reduced, labels, order, n)\n",
    "        #print(X_te.shape)\n",
    "        #print(X_te)\n",
    "        #Scale the data to 0-mean and 1-variance\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_std = scaler.fit_transform(X_tr)\n",
    "        X_te_std = scaler.transform(X_te)\n",
    "        #print(n)\n",
    "        #print(X_tr_std.shape)\n",
    "        #print(X_te_std.shape)\n",
    "        #Create the model\n",
    "        final_clf = svm.SVC(C = 1, kernel = 'linear', gamma = 'auto')\n",
    "        #SVM fit on the training data (reduced and scaled)\n",
    "        final_clf.fit(X_tr_std, y_tr)\n",
    "        #performances on test\n",
    "        tot_perf.append(final_clf.score(X_te_std, y_te))\n",
    "        \n",
    "\n",
    "    #plot the figure\n",
    "    plt.figure(figsize=(10, 7), dpi=80)\n",
    "    plt.errorbar(range(start, X.shape[0]-4, 2) , tot_perf, label=\"Error bars plot\", fmt=\"s-\",  linewidth=3)\n",
    "    #plt.errorbar(range(40)+1, accuracy_train_mean, label=\"Error bars plot\", fmt=\"s-\",  linewidth=3)\n",
    "    plt.grid(b=True, which='major', color='k', linestyle='--', alpha = 0.4)\n",
    "    plt.minorticks_on()\n",
    "    plt.title('SVM perfomances over different train/test dataset of reduced features')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Train instances considered')\n",
    "\n",
    "    plt.legend(['Test Accuracy', 'Train Accuracy'], loc=4)\n",
    "    name='test_acc_CORR_'+Model_Name+'.eps'\n",
    "    plt.savefig(name, format='eps')\n",
    "    plt.show()\n",
    "    \n",
    "    X_tr, X_te, y_tr, y_te = get_experimental_sequence(feature_reduced, labels, order, 19)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_tr_std = scaler.fit_transform(X_tr)\n",
    "    X_te_std = scaler.transform(X_te)\n",
    "\n",
    "    #Create the model\n",
    "    final_clf = svm.SVC(C = 1, kernel = 'linear', gamma = 'auto')\n",
    "    #SVM fit on the training data (reduced and scaled)\n",
    "    final_clf.fit(X_tr_std, y_tr)\n",
    "    #performances on test\n",
    "    one_session_train_score = final_clf.score(X_te_std, y_te)\n",
    "    print('Score on one session of training data (20 questions) for ' +str(Model_Name) +': ' + str(one_session_train_score))\n",
    "    \n",
    "    \n",
    "def get_experimental_sequence(X, y, order, c):\n",
    "    idx_tr = []\n",
    "    y_tr = []\n",
    "    half = int(X.shape[0]/2)\n",
    "    #c = number of minimum instances\n",
    "    i=0\n",
    "    j=0\n",
    "    for element in order:\n",
    "        if have_two_classes(y_tr) and len(y_tr)>c:\n",
    "            break\n",
    "        if element==1: # 1 is YES\n",
    "            idx_tr.append(i)\n",
    "            #X_tr.append(X[i,:])\n",
    "            y_tr.append(y[i])\n",
    "            i+=1\n",
    "        else: # 0 is NO\n",
    "            idx_tr.append(j+half)\n",
    "            #X_tr.append(X[half+j,:])\n",
    "            y_tr.append(y[half+j])\n",
    "            j+=1\n",
    "            \n",
    "    X_tr = X[idx_tr,:]\n",
    "    y_tr = y[idx_tr]\n",
    "    \n",
    "    idx_te = [k for k in range(X.shape[0])]\n",
    "    for l in idx_tr:\n",
    "        idx_te.remove(l)\n",
    "    X_te = X[idx_te,:]\n",
    "    y_te = y[idx_te]\n",
    "    \n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "def have_two_classes(y):\n",
    "    have_zero = False\n",
    "    have_one = False\n",
    "    for element in y: \n",
    "        if element==1: have_zero = True\n",
    "    for element in y: \n",
    "        if element==0: have_one = True\n",
    "    return have_zero & have_one\n",
    "\n",
    "\n",
    "def extract_dataset(destination_folder):\n",
    "    #Import data from mat files\n",
    "    old_path=os.getcwd()\n",
    "    os.chdir(destination_folder)\n",
    "    yes_EEG_contents = sio.loadmat('EEGyes.mat')\n",
    "    no_EEG_contents = sio.loadmat('EEGno.mat')\n",
    "\n",
    "    channels_no_EEG=no_EEG_contents[\"EEGno\"]\n",
    "    channels_yes_EEG=yes_EEG_contents[\"EEGyes\"]\n",
    "\n",
    "    #Features Loading\n",
    "    features_extracted_yes   = sio.loadmat('FeaturesYes.mat')['FeaturesYes']\n",
    "    features_extracted_no    = sio.loadmat('FeaturesNO.mat')['FeaturesNo']\n",
    "    connectivity_feature_yes = sio.loadmat('ConnectivityFeaturesYes.mat')['ConnectivityFeaturesYes']\n",
    "    connectivity_feature_no  = sio.loadmat('ConnectivityFeaturesNo.mat')['ConnectivityFeaturesNo']\n",
    "\n",
    "    channels_structure_yes_EEG = channels_to_vector(channels_yes_EEG)\n",
    "    channels_structure_no_EEG  = channels_to_vector(channels_no_EEG)\n",
    "\n",
    "    ##Structuring of the data:\n",
    "    #the code below create the train matrix with respect to the signal given in \"channel_structure\" but using the features contained in \"features_extracted*\" and in \"connettivity_feature*\".\n",
    "    feature_dataset_yes_EEG, EEG_yes_labels = get_feature_matrix_and_labels(channels_structure_yes_EEG,\"Yes\",features_extracted_yes,connectivity_feature_yes);\n",
    "\n",
    "    feature_dataset_no_EEG, EEG_no_labels = get_feature_matrix_and_labels(channels_structure_no_EEG,\"No\",features_extracted_no,connectivity_feature_no);\n",
    "\n",
    "    #Merge the labeled data\n",
    "    feature_dataset_full = np.concatenate((feature_dataset_yes_EEG, feature_dataset_no_EEG), axis=0 )\n",
    "    labels = np.concatenate((EEG_yes_labels,EEG_no_labels), axis=0)\n",
    "    \n",
    "    #Order\n",
    "    order = sio.loadmat('order.mat')['order']\n",
    "    order = order.flatten()\n",
    "\n",
    "    os.chdir(old_path)\n",
    "    \n",
    "   \n",
    "\n",
    "    return feature_dataset_full, labels, order\n",
    "\n",
    "def select_k_iid(X, y):\n",
    "    feature_dataset_full = X\n",
    "    labels = y\n",
    "    tot_perf = []\n",
    "    end_feat = 100\n",
    "    for k in range(1,end_feat):\n",
    "        #print report\n",
    "        #sys.stdout.write(\"\\033[F\") # Cursor up one line\n",
    "        #sys.stdout.write(\"\\033[K\") # Clear to the end of line\n",
    "        #print('progress: ' + str(k/end_feat*100) + ' %')\n",
    "\n",
    "\n",
    "        #reduce with anova f-test score the data in k features\n",
    "        reducer = SelectKBest(f_classif, k)\n",
    "        feature_reduced = reducer.fit_transform(feature_dataset_full, labels)\n",
    "\n",
    "\n",
    "        clf = svm.SVC(C = 1, kernel = 'linear', gamma = 'auto') #Parameters of the SVM\n",
    "        fraction_train_test = 0.30 #Division tran/test of the full dataset\n",
    "        num_experiments = 500 #Number of different partition for better evaluating the performances\n",
    "\n",
    "        #Function that actually computes the performances on the k features \n",
    "        perf_temp = classification_SVM_experiments_std(feature_reduced, labels, clf, fraction_train_test, num_experiments,Model_Name,0) #Note: this method scale the data to have 0-mean and unit variance\n",
    "        tot_perf.append(np.mean(perf_temp)) #Save the result\n",
    "\n",
    "\n",
    "    best_k = np.argmax(tot_perf)+1\n",
    "    print(\"smallest k that gives best low train results:\")\n",
    "    print(best_k)\n",
    "    print()\n",
    "    print(\"wich have lead to the top performance:\")\n",
    "    print(np.max(tot_perf))\n",
    "    return best_k\n",
    "\n",
    "def select_k_temporal(X, y, order, n):\n",
    "    feature_dataset_full = X\n",
    "    labels = y\n",
    "    tot_perf = []\n",
    "    end_feat = 100\n",
    "    for k in range(1,end_feat):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #X_tr, X_te, y_tr, y_te = split_train_test_temporal(feature_reduced, labels, 10)\n",
    "        X_tr, X_te, y_tr, y_te = get_experimental_sequence(feature_dataset_full, labels, order, n)\n",
    "        \n",
    "        #reduce with anova f-test score the data in k features\n",
    "        reducer = SelectKBest(f_classif, k)\n",
    "        X_tr = reducer.fit_transform(X_tr, y_tr)\n",
    "        X_te = reducer.transform(X_te)\n",
    "        \n",
    "        #Scale the data to 0-mean and 1-variance\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_std = scaler.fit_transform(X_tr)\n",
    "        X_te_std = scaler.transform(X_te)\n",
    "\n",
    "        #Create the model\n",
    "        final_clf = svm.SVC(C = 1, kernel = 'linear', gamma = 'auto')\n",
    "        #SVM fit on all the available data (reduced and scaled)\n",
    "        final_clf.fit(X_tr_std, y_tr)\n",
    "        #performances\n",
    "        tot_perf.append(final_clf.score(X_te_std, y_te))\n",
    "\n",
    "\n",
    "\n",
    "    best_k = np.argmax(tot_perf)+1\n",
    "    print(\"smallest k that gives best low train results:\")\n",
    "    print(best_k)\n",
    "    print()\n",
    "    print(\"wich have lead to the top performance:\")\n",
    "    print(np.max(tot_perf))\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting stuff:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of results\n",
    "\n",
    "Here we use 20 instaces for train (one session) and the remaining for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dataset gathered on: Day_1\n",
      "smallest k that gives best low train results:\n",
      "36\n",
      "\n",
      "wich have lead to the top performance:\n",
      "0.666666666667\n",
      "Training made on 20 instances\n",
      "Test made on 60 instances\n",
      "Score for Day_1: 0.616666666667\n",
      "\n",
      "\n",
      "Accuracy on dataset gathered on: Day_2\n",
      "smallest k that gives best low train results:\n",
      "5\n",
      "\n",
      "wich have lead to the top performance:\n",
      "0.666666666667\n",
      "Training made on 20 instances\n",
      "Test made on 60 instances\n",
      "Score for Day_2: 0.616666666667\n",
      "\n",
      "\n",
      "Accuracy on dataset gathered on: Day_3\n",
      "smallest k that gives best low train results:\n",
      "2\n",
      "\n",
      "wich have lead to the top performance:\n",
      "0.7\n",
      "Training made on 20 instances\n",
      "Test made on 20 instances\n",
      "Score for Day_3: 0.7\n",
      "\n",
      "\n",
      "Accuracy on dataset gathered on: Day_4\n",
      "smallest k that gives best low train results:\n",
      "67\n",
      "\n",
      "wich have lead to the top performance:\n",
      "0.55\n",
      "Training made on 20 instances\n",
      "Test made on 60 instances\n",
      "Score for Day_4: 0.6\n",
      "\n",
      "\n",
      "Accuracy on dataset gathered on: Day_5\n",
      "smallest k that gives best low train results:\n",
      "25\n",
      "\n",
      "wich have lead to the top performance:\n",
      "0.925\n",
      "Training made on 20 instances\n",
      "Test made on 40 instances\n",
      "Score for Day_5: 0.95\n",
      "\n",
      "\n",
      "Accuracy on dataset gathered on: Day_6\n",
      "smallest k that gives best low train results:\n",
      "5\n",
      "\n",
      "wich have lead to the top performance:\n",
      "0.65\n",
      "Training made on 20 instances\n",
      "Test made on 60 instances\n",
      "Score for Day_6: 0.616666666667\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "########################### INPUT PARAMETERS########\n",
    "\n",
    "#Select the traine  classifier\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "#Select the input folders to test the model\n",
    "Input_Path_Folders=['./DataDay1','./DataDay2','./DataDay3','./DataDay4','./DataDay5','./DataDay6']\n",
    "i = 1\n",
    "n=19 # number of questions -1\n",
    "#print('Model trained: ')\n",
    "for single_input in Input_Path_Folders:\n",
    "    day = 'Day_'+str(i)\n",
    "    print('Accuracy on dataset gathered on: ' + day)\n",
    "    [feature_dataset_full, labels, order]= extract_dataset(single_input)\n",
    "    best_k_temporal = select_k_temporal(feature_dataset_full, labels, order, n) #Best K iid\n",
    "    \n",
    "    #Reduce dimensions\n",
    "    reducer = SelectKBest(f_classif, best_k_temporal)\n",
    "    feature_dataset_reduced = reducer.fit_transform(feature_dataset_full, labels)\n",
    "    \n",
    "    X_tr, X_te, y_tr, y_te = get_experimental_sequence(feature_dataset_reduced, labels, order, n)\n",
    "    print('Training made on ' + str(X_tr.shape[0])+ ' instances')\n",
    "    print('Test made on ' + str(X_te.shape[0])+ ' instances')\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_std = scaler.fit_transform(X_tr)\n",
    "    X_te_std = scaler.transform(X_te)\n",
    "\n",
    "    #Create the model\n",
    "    final_clf = svm.SVC(C = 1, kernel = 'linear', gamma = 'auto')\n",
    "    #SVM fit on the training data (reduced and scaled)\n",
    "    final_clf.fit(X_tr_std, y_tr)\n",
    "    #performances on test\n",
    "    one_session_train_score = final_clf.score(X_te_std, y_te)\n",
    "    print('Score for ' +str(day) +': ' + str(one_session_train_score))\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the performances with 2 session of training\n",
    "\n",
    "Note: day 3 was excluded because it has only 2 session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dataset gathered on: ./DataDay1\n",
      "smallest k that gives best low train results:\n",
      "6\n",
      "\n",
      "wich have lead to the top performance:\n",
      "0.7\n",
      "Training made on 40 instances\n",
      "Test made on 40 instances\n",
      "Score for ./DataDay1: 0.75\n",
      "\n",
      "\n",
      "Accuracy on dataset gathered on: ./DataDay2\n",
      "smallest k that gives best low train results:\n",
      "22\n",
      "\n",
      "wich have lead to the top performance:\n",
      "0.65\n",
      "Training made on 40 instances\n",
      "Test made on 40 instances\n",
      "Score for ./DataDay2: 0.55\n",
      "\n",
      "\n",
      "Accuracy on dataset gathered on: ./DataDay4\n",
      "smallest k that gives best low train results:\n",
      "28\n",
      "\n",
      "wich have lead to the top performance:\n",
      "0.775\n",
      "Training made on 40 instances\n",
      "Test made on 40 instances\n",
      "Score for ./DataDay4: 0.9\n",
      "\n",
      "\n",
      "Accuracy on dataset gathered on: ./DataDay5\n",
      "smallest k that gives best low train results:\n",
      "23\n",
      "\n",
      "wich have lead to the top performance:\n",
      "1.0\n",
      "Training made on 40 instances\n",
      "Test made on 20 instances\n",
      "Score for ./DataDay5: 1.0\n",
      "\n",
      "\n",
      "Accuracy on dataset gathered on: ./DataDay6\n",
      "smallest k that gives best low train results:\n",
      "45\n",
      "\n",
      "wich have lead to the top performance:\n",
      "0.925\n",
      "Training made on 40 instances\n",
      "Test made on 40 instances\n",
      "Score for ./DataDay6: 0.95\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "########################### INPUT PARAMETERS########\n",
    "\n",
    "#Select the traine  classifier\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "#Select the input folders to test the model\n",
    "Input_Path_Folders=['./DataDay1','./DataDay2','./DataDay4','./DataDay5','./DataDay6']\n",
    "\n",
    "n = 39\n",
    "#print('Model trained: ')\n",
    "for single_input in Input_Path_Folders:\n",
    "    #day = 'Day_'+str(i)\n",
    "    day = single_input\n",
    "    print('Accuracy on dataset gathered on: ' + day)\n",
    "    [feature_dataset_full, labels, order]= extract_dataset(single_input)\n",
    "    best_k_temporal = select_k_temporal(feature_dataset_full, labels, order, n) #Best K iid\n",
    "    \n",
    "    #Reduce dimensions\n",
    "    reducer = SelectKBest(f_classif, best_k_temporal)\n",
    "    feature_dataset_reduced = reducer.fit_transform(feature_dataset_full, labels)\n",
    "    \n",
    "    X_tr, X_te, y_tr, y_te = get_experimental_sequence(feature_dataset_reduced, labels, order, n)\n",
    "    print('Training made on ' + str(X_tr.shape[0])+ ' instances')\n",
    "    print('Test made on ' + str(X_te.shape[0])+ ' instances')\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_std = scaler.fit_transform(X_tr)\n",
    "    X_te_std = scaler.transform(X_te)\n",
    "\n",
    "    #Create the model\n",
    "    final_clf = svm.SVC(C = 1, kernel = 'linear', gamma = 'auto')\n",
    "    #SVM fit on the training data (reduced and scaled)\n",
    "    final_clf.fit(X_tr_std, y_tr)\n",
    "    #performances on test\n",
    "    one_session_train_score = final_clf.score(X_te_std, y_te)\n",
    "    print('Score for ' +str(day) +': ' + str(one_session_train_score))\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
